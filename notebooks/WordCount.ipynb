{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Wordcount with Rheem <div style=\"float:right; z-index:1\"><img src=\"rheem.png\" width=\"100px\" /></div></h1>\n",
    "\n",
    "This notebook demonstrates how to run Wordcount, the _Hello world!_ for data processing tools. To run this notebook, you will need the [Jupyter Scala kernel](https://github.com/alexarchambault/jupyter-scala)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us briefly give the basic idea of WordCount in the Java API:\n",
    "\n",
    "```java\n",
    "RheemContext rheemCtx = new RheemContext(new Configuration())\n",
    "    .withPlugin(Spark.basicPlugin())\n",
    "    .withPlugin(Java.basicPlugin());\n",
    "    \n",
    "JavaPlanBuilder planBuilder = new JavaPlanBuilder(rheemCtx);\n",
    "Collection<Tuple2<String, Integer> wordCounts = planBuilder\n",
    "    .readTextFile(\"hdfs://my-namenode/my-corpus.txt\")\n",
    "    .flatMap(line -> Arrays.asList(line.split(\"\\\\W+\")))\n",
    "    .map(word -> new Tuple2<>(word.toLowerCase(), 1))\n",
    "    .reduceByKey(Tuple2::getField0,\n",
    "                 (t1, t2) -> new Tuple2<>(t1.getField0(), t1.getField1() + t2.getField1()))\n",
    "    .collect();\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At first, we obtain an input dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "locally {\n",
    "    import java.io._\n",
    "    import scala.io.Source\n",
    "    \n",
    "    val file = new File(\"data/iliad.txt\")\n",
    "    if (!file.exists) {\n",
    "        file.getParentFile.mkdirs()\n",
    "        val source = Source.fromURL(\"http://www.gutenberg.org/cache/epub/6130/pg6130.txt\")\n",
    "        val writer = new BufferedWriter(new OutputStreamWriter(new FileOutputStream(file), \"UTF-8\"))\n",
    "        source.foreach(char => writer.write(char.asInstanceOf[Int]))\n",
    "        writer.close()\n",
    "        source.close()\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we intialize Rheem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[32mimport \u001b[39m\u001b[36m$ivy.$                           \n",
       "\u001b[39m"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Disable logging.\n",
    "import $ivy.`org.slf4j:slf4j-nop:1.7.12`\n",
    "org.slf4j.LoggerFactory.getLogger(\"root\").info(\"Enforcing slf4j-nop...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "// Load dependencies into the kernel.\n",
    "import $ivy.`org.qcri.rheem::rheem-api:0.3.0`,\n",
    "    $ivy.`org.qcri.rheem:rheem-basic:0.3.0`,\n",
    "    $ivy.`org.qcri.rheem:rheem-java:0.3.0`,\n",
    "    $ivy.`org.qcri.rheem::rheem-spark:0.3.0`,\n",
    "    $ivy.`org.apache.spark::spark-core:1.6.0`,\n",
    "    $ivy.`org.apache.spark::spark-graphx:1.6.0`,\n",
    "    $ivy.`de.hpi.isg:profiledb-store:0.1.1`,\n",
    "    $ivy.`com.github.sekruse::spark-summit-demo:1.0-SNAPSHOT`\n",
    "\n",
    "// Do the relevant imports.\n",
    "import org.qcri.rheem.api._\n",
    "import org.qcri.rheem.core.api._\n",
    "import org.qcri.rheem.core.optimizer.ProbabilisticDoubleInterval\n",
    "import org.qcri.rheem.java.Java, org.qcri.rheem.spark.Spark\n",
    "import de.hpi.isg.profiledb.store.model._\n",
    "import com.github.sekruse.spark_summit_demo._\n",
    "\n",
    "// Set up a Rheem context.\n",
    "val localDir = new java.io.File(\".\").getAbsoluteFile\n",
    "val config = new Configuration(s\"file://$localDir/rheem.properties\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If this notebook is run in an offline environment, run the `run-webserver.sh` script to provide the required JS libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "val offline = true\n",
    "if (offline) {\n",
    "    addModule(\"plotly\", \"http://localhost:8888/files/js/plotly-latest.min\")\n",
    "    addModule(\"d3\", \"http://localhost:8888/files/js/d3.v4.min\")\n",
    "    config.setProperty(\"spark.driver.host\", \"localhost\")\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can run the Wordcount."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "// Define a class to handle word counts neatly.\n",
    "case class WC(word: String, count: Int) {\n",
    "    def +(that: WC) = {\n",
    "        require(this.word == that.word)\n",
    "        WC(this.word, this.count + that.count)\n",
    "    }\n",
    "\n",
    "    override def toString: String = s\"${count}x ${word}\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "locally {\n",
    "    val experiment = new Experiment(\"my-exp\", new Subject(\"WordCount\", \"1.0\"))\n",
    "    val rheemCtx = new RheemContext(config)\n",
    "        .withPlugin(Java.basicPlugin)\n",
    "        .withPlugin(Spark.basicPlugin)\n",
    "    \n",
    "    // Set up a new plan.\n",
    "    val planBuilder = new PlanBuilder(rheemCtx)\n",
    "        .withJobName(\"WordCount\")\n",
    "        .withUdfJarsOf(this.getClass)\n",
    "        .withExperiment(experiment)\n",
    "    \n",
    "    val wordCounts = planBuilder\n",
    "\n",
    "        // Read the text file.\n",
    "        .readTextFile(s\"file://$localDir/data/iliad.txt\").withName(\"Load file\")\n",
    "\n",
    "        // Split each line by non-word characters.\n",
    "        .flatMap(_.split(\"\\\\W+\"), udfLoad = (in: Long, out: Long) => 100 * in).withName(\"Split words\")\n",
    "        .withTargetPlatforms(Spark.platform)\n",
    "\n",
    "        // Filter empty tokens.\n",
    "        .filter(_.nonEmpty, selectivity = 0.99).withName(\"Filter empty words\")\n",
    "\n",
    "        // Attach counter to each word.\n",
    "        .map(word => WC(word.toLowerCase, 1)).withName(\"To lower case, add counter\")\n",
    "\n",
    "        // Sum up counters for every word.\n",
    "        .reduceByKey(_.word, _ + _).withName(\"Add counters\")\n",
    "        .withCardinalityEstimator((in: Long) => math.round(in * 0.01))\n",
    "        //.withTargetPlatforms(Spark.platform)\n",
    "    \n",
    "        // Mask rather small words counts.\n",
    "        .map(wc => if (wc.count > 1000) wc else WC(\"(other)\", wc.count)).withName(\"Mask rather small words\")\n",
    "        .reduceByKey(_.word, _ + _).withName(\"Add counters again\")\n",
    "\n",
    "        // Execute the plan and collect the results.\n",
    "        .collect()\n",
    "    \n",
    "    publish.html(\"<h1>Words in Homer's Iliad</h1>\")\n",
    "    plotPieChart[WC](\n",
    "        name = \"Words in Homer's Iliad\",\n",
    "        data = wordCounts,\n",
    "        values = _.count.toDouble,\n",
    "        labels = _.word,\n",
    "        showlegend = false\n",
    "    )\n",
    " \n",
    "    publish.html(\"<h1>Execution plan</h1>\")\n",
    "    plotExecutionPlan(experiment)\n",
    "}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Scala",
   "language": "scala",
   "name": "scala"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "mimetype": "text/x-scala",
   "name": "scala211",
   "nbconvert_exporter": "scala",
   "pygments_lexer": "scala",
   "version": "2.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
